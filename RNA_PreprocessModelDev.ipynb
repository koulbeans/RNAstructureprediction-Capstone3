{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46922d87-1453-4668-8080-744460ee9fa4",
   "metadata": {},
   "source": [
    "# Springboard Capstone 3: Preprocessing & Modeling\n",
    "## RNA Secondary Structure Prediction\n",
    "\n",
    "Data source: https://www.kaggle.com/competitions/stanford-ribonanza-rna-folding/data<br>\n",
    "Using the QUICK START file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43c90e0d-29fb-4cd8-bfe0-ec1058e36ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19004839-cdb2-4385-95fc-75f17d74f2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "ogdata = pd.read_csv('train_data_QUICK_START.csv')\n",
    "sequences = ogdata[['sequence_id', 'sequence']].drop_duplicates().reset_index(drop = True)\n",
    "\n",
    "# define max sequence length (for padding & shape purposes)\n",
    "maxlength = max([len(x) for x in sequences['sequence']])\n",
    "\n",
    "# split off test set\n",
    "train_seqs, test_seqs = train_test_split(sequences, random_state=66)\n",
    "train_seqs.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba16c9b8-51b8-498f-8a44-15e3edd7aac0",
   "metadata": {},
   "source": [
    "We will use the `sequence_id` as the unique identifier. Note that initial data set has two lines for each sequence_id corresponding to the two probes (DMS, which modifies bace-pairing surface, and 2A3, which modifies backbone).\n",
    "\n",
    "The `test_seqs` set split off here will be left until the final step of this notebook, with intermediate steps doing their own further subsetting of the `train_seqs` or other data sets to validate smaller parts of the processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fb1c1d5-2a87-4169-b28d-6be0f88e59f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "  def neighbor_finder(base, distance, sequence_series):\n",
    "    '''Creates a list of the neighbor base identities located the specified distance away\n",
    "    in the sequence from the argument base. Distance of -1 means immediately preceeding (5'),\n",
    "    while distance of 1 means immediately following (3').'''\n",
    "    all_values = []\n",
    "    for sequence in sequence_series:\n",
    "        neighbors = (sequence[i + distance] for i, ltr in enumerate(sequence) if ltr == base)\n",
    "        all_values += list(neighbors)\n",
    "    return all_values\n",
    "\n",
    "def populate_neighbors(base, distance_list, sequence_series):\n",
    "    '''Creates a dataframe for a particular base specifying neighbors at positions indicated in\n",
    "    distance_list using neighbor_finder function.'''\n",
    "    df = pd.DataFrame()\n",
    "    for dist in distance_list:\n",
    "        try:\n",
    "            df['base_'+str(dist)] = neighbor_finder(base, dist, sequence_series)\n",
    "        except IndexError:\n",
    "            df['base_'+str(dist)] = 'None'\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e18e201-9c83-4612-84af-4ff4a40c0a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_list = [-2, -1, 1, 2]\n",
    "\n",
    "dfA_neighbors = populate_neighbors('A', distance_list, train_seqs['sequence'])\n",
    "dfC_neighbors = populate_neighbors('C', distance_list, train_seqs['sequence'])\n",
    "dfG_neighbors = populate_neighbors('G', distance_list, train_seqs['sequence'])\n",
    "dfU_neighbors = populate_neighbors('U', distance_list, train_seqs['sequence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96ec7970-f40c-4e43-a99b-a15abe6a491c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findbase(base, sequence):\n",
    "    '''Generator with all indices where a given base is found in the string RNA sequence'''\n",
    "    return (i for i, ltr in enumerate(sequence) if ltr == base)\n",
    "\n",
    "def findbaseDF(base, series):\n",
    "    '''Iterates the findbase function over a pandas series or equivalent'''\n",
    "    all_indices = []\n",
    "    for seq in series:\n",
    "        idx = findbase(base, seq)\n",
    "        all_indices += list(idx)\n",
    "    return all_indices\n",
    "\n",
    "def vals_from_df(base, dataframe, sequence_series):\n",
    "    '''Takes dataframe where each row is one RNA sample and each column is some value or readout for a position\n",
    "    in the sequence and extracts it into one long list that can serve as feature column for other dataframe.'''\n",
    "    all_values = []\n",
    "    for i, seq in enumerate(sequence_series):\n",
    "        idx = findbase(base, seq)\n",
    "        values = dataframe.iloc[i, idx]\n",
    "        all_values += list(values)\n",
    "    return all_values\n",
    "\n",
    "def populate_properties(base, columns, source_dict):\n",
    "    '''Uses the above functions to create a dataframe for a particular base containing data for the properties\n",
    "    outlined as keys of the source_dict from the sources indicated in the dictionary.'''\n",
    "    df = pd.DataFrame(columns = columns)\n",
    "    for property in source_dict.keys():\n",
    "        if property == 'position':\n",
    "            df[property] = findbaseDF(base, source_dict[property])\n",
    "        else:\n",
    "            df[property] = vals_from_df(base, source_dict[property], source_dict['position'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3db7ae3-f251-4035-a9de-fe0d8e293528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for data source prep\n",
    "def viennaPU(viennaDF, seq_ids):\n",
    "    '''Get Vienna secondary structure predictions, read out as P (paired) or U (unpaired) from a\n",
    "    dataframe with an 'ss' column for the sequences in list/series seq_ids.'''\n",
    "    DF = viennaDF.loc[viennaDF['sequence_id'].isin(seq_ids)]\n",
    "    \n",
    "    #tokenize\n",
    "    ssVienna = DF['ss'].apply(list)\n",
    "    \n",
    "    # pad\n",
    "    ssVienna_padded = ssVienna.apply(lambda x: x + ['N']*(maxlength - len(x)))\n",
    "    \n",
    "    ssVienna_pdexp =  pd.DataFrame(np.vstack(ssVienna_padded))\n",
    "    ssViennaDF = ssVienna_pdexp.replace({'(': 'P', ')': 'P', '.': 'U'})\n",
    "\n",
    "    return ssViennaDF\n",
    "\n",
    "def get_reactivity(dataframe, probe, na_val = 0.3):\n",
    "    df = dataframe[dataframe['experiment_type'] == str(probe)+'_MaP']\n",
    "    reactivity_cols = [col for col in dataframe.columns if 'reactivity' in col]\n",
    "    df = df[reactivity_cols].fillna(na_val)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebbbe228-b1e1-4e44-913d-632a14af31d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get Vienna data for training set\n",
    "viennaDF = pd.read_pickle('viennaDF.pkl')\n",
    "\n",
    "ssViennaDF_train = viennaPU(viennaDF, train_seqs['sequence_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2761321d-371a-4ad3-b502-efea116f00d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = ogdata.loc[ogdata['sequence_id'].isin(train_seqs['sequence_id'])]\n",
    "twoA3reactDF_train = get_reactivity(data_train, '2A3')\n",
    "dmsReactDF_train = get_reactivity(data_train, 'DMS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d7a2299-704d-4fea-8e5a-f3e89508d5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['position', 'paired', '2A3_react', 'DMS_react']\n",
    "source_dict = {'position': train_seqs['sequence'], 'paired': ssViennaDF_train, '2A3_react': twoA3reactDF_train, 'DMS_react': dmsReactDF_train}\n",
    "\n",
    "dfG_props = populate_properties('G', columns, source_dict)\n",
    "dfA_props = populate_properties('A', columns, source_dict)\n",
    "dfC_props = populate_properties('C', columns, source_dict)\n",
    "dfU_props = populate_properties('U', columns, source_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ea93566-ac3f-4b37-96de-2c45c1459027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all of the by-base property dataframes\n",
    "df_props = pd.concat([dfA_props, dfC_props, dfG_props, dfU_props], keys = ['A', 'C', 'G', 'U'])\n",
    "df_props.index.rename(['base', 'index'], inplace = True)\n",
    "df_props.reset_index(level = 0, inplace = True)\n",
    "df_props.reset_index(inplace = True, drop = True)\n",
    "\n",
    "# combine all of the neighbor dataframes\n",
    "df_neighbors = pd.concat([dfA_neighbors, dfC_neighbors, dfG_neighbors, dfU_neighbors],\\\n",
    "                         keys = ['A', 'C', 'G', 'U'])\n",
    "df_neighbors.index.rename(['base', 'index'], inplace = True)\n",
    "df_neighbors.reset_index(level = 0, inplace = True)\n",
    "df_neighbors.reset_index(inplace = True, drop = True)\n",
    "\n",
    "# merge both into one\n",
    "df = df_neighbors.join(df_props, rsuffix='_props')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12bee709-eafc-4e18-a448-4fd5228fa05c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  base base_-2 base_-1 base_1 base_2  position paired  2A3_react  DMS_react\n",
      "0    A       G       G      A   None         3      U        0.3        0.3\n",
      "1    A       G       A      C   None         4      U        0.3        0.3\n",
      "2    A       C       G      C   None         7      P        0.3        0.3\n",
      "3    A       C       G      G   None        12      U        0.3        0.3\n",
      "4    A       G       U      G   None        15      U        0.3        0.3\n"
     ]
    }
   ],
   "source": [
    "# check that base and base_props are the same and drop base_props\n",
    "\n",
    "if len(df['base'].compare(df['base_props'])) == 0:\n",
    "    df.drop('base_props', axis =1, inplace = True)\n",
    "    print(df.head())\n",
    "else:\n",
    "    print(\"Oops, something went wrong when making the dataframe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6666fa81-2bfa-4904-b810-c0da3a366cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to pickle and unpickle\n",
    "# comment out and uncomment as appropriate\n",
    "\n",
    "df.to_pickle('df_train.pkl')\n",
    "\n",
    "df_train = df\n",
    "\n",
    "test_seqs.to_pickle('test_seqs.pkl')\n",
    "\n",
    "# df_train = pd.read_pickle('df_train.pkl')\n",
    "# seq_split = pd.read_pickle('sequence_split.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ee1383-0710-4444-b7ce-540db0ef6691",
   "metadata": {},
   "source": [
    "We now have a dataframe with one row per base, containing (hopefully) most of the pertinent information for predicting the reactivities. Now we just need to split into features and targets and one-hot encode all of categorical variables. (Could map to numeric values instead, but that would introduce an ordinal property/component that is not applicable.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9825a883-a5e8-4889-99ca-566026f39f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features\n",
    "features_train = df_train.drop(['2A3_react', 'DMS_react'], axis = 1)\n",
    "features_train = pd.get_dummies(features_train)\n",
    "features_train = features_train.astype(np.float32)\n",
    "\n",
    "# targets\n",
    "target_train = df_train[['2A3_react', 'DMS_react']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a539fb6a-b3c2-431b-8e9e-9f0dd88d85dc",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "Preprocessing done. Now let's try out a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "86800984-4f93-4132-9ecd-5870cbb23110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\dkoul\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import keras modules\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dabbc80c-70f6-4f6a-980a-f1f4f7ac882b",
   "metadata": {},
   "outputs": [],
   "source": [
    "numfeatures = len(features_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eb6d0294-9989-439a-acc2-4d37a939817c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\dkoul\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\dkoul\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Input(shape=(None, numfeatures)))\n",
    "model.add(Dense(50))\n",
    "model.add(Dense(50))\n",
    "model.add(Dense(2))\n",
    "model.compile(optimizer = 'adam', loss = 'mae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f9297cb-2c7f-4f86-8da0-cba0979e0085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, None, 50)          1350      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, None, 50)          2550      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, None, 2)           102       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4002 (15.63 KB)\n",
      "Trainable params: 4002 (15.63 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "59c98461-0ca0-4be1-96c0-0f3d0fda29bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "WARNING:tensorflow:From C:\\Users\\dkoul\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "58954/58954 [==============================] - 63s 1ms/step - loss: 0.2485 - val_loss: 0.2634\n",
      "Epoch 2/5\n",
      "58954/58954 [==============================] - 61s 1ms/step - loss: 0.2096 - val_loss: 0.2631\n",
      "Epoch 3/5\n",
      "58954/58954 [==============================] - 63s 1ms/step - loss: 0.2096 - val_loss: 0.2631\n",
      "Epoch 4/5\n",
      "58954/58954 [==============================] - 62s 1ms/step - loss: 0.2096 - val_loss: 0.2631\n",
      "Epoch 5/5\n",
      "58954/58954 [==============================] - 62s 1ms/step - loss: 0.2096 - val_loss: 0.2631\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x293dfa1ddd0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(features_train, target_train, batch_size = 300,\\\n",
    "          validation_split = 0.20, epochs = 5, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "744fd24e-8fcb-4f67-83eb-0cf51b15fa94",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model1.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a75143e-7341-4cb6-b2f4-05d8e2c44a19",
   "metadata": {},
   "source": [
    "That actually looks pretty good! A thorough exploration of hyperparameter space (not to mention possible model architecture space) would be prohibitively computationally expensive, so instead we will do a coarse grain OFAT look to see if we can identify one or more parameters that would lead to model improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "600b86ac-8bd0-42ef-8932-d3d80ed157dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "294770/294770 [==============================] - 289s 977us/step - loss: 0.2096 - val_loss: 0.2631\n",
      "Epoch 2/5\n",
      "294770/294770 [==============================] - 298s 1ms/step - loss: 0.2096 - val_loss: 0.2631\n",
      "Epoch 3/5\n",
      "294770/294770 [==============================] - 291s 986us/step - loss: 0.2096 - val_loss: 0.2631\n",
      "Epoch 4/5\n",
      "294770/294770 [==============================] - 285s 968us/step - loss: 0.2096 - val_loss: 0.2631\n",
      "Epoch 5/5\n",
      "294770/294770 [==============================] - 288s 976us/step - loss: 0.2096 - val_loss: 0.2631\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x293dead1290>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# smaller batch size\n",
    "model.fit(features_train, target_train, batch_size = 60,\\\n",
    "          validation_split = 0.20, epochs = 5, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "20512040-4e99-4f84-a76a-c02c2db0a23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model1_smallbatch.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d14528f2-d0ba-4423-bf2c-b64a6453c098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "58949/58949 [==============================] - 73s 1ms/step - loss: 0.2473 - val_loss: 0.2633\n",
      "Epoch 2/5\n",
      "58949/58949 [==============================] - 72s 1ms/step - loss: 0.2097 - val_loss: 0.2633\n",
      "Epoch 3/5\n",
      "58949/58949 [==============================] - 72s 1ms/step - loss: 0.2097 - val_loss: 0.2633\n",
      "Epoch 4/5\n",
      "58949/58949 [==============================] - 72s 1ms/step - loss: 0.2097 - val_loss: 0.2633\n",
      "Epoch 5/5\n",
      "58949/58949 [==============================] - 71s 1ms/step - loss: 0.2097 - val_loss: 0.2657\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x21e62943490>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of nodes in dense layers\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(Input(shape=(None, numfeatures)))\n",
    "model2.add(Dense(100))\n",
    "model2.add(Dense(100))\n",
    "model2.add(Dense(2))\n",
    "model2.compile(optimizer = 'adam', loss = 'mae')\n",
    "\n",
    "model2.fit(features_train, target_train, batch_size = 300,\\\n",
    "          validation_split = 0.20, epochs = 5, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0ff4b660-ff59-49fe-8164-25537fa90ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.save('model2.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e3b3d197-3ec2-4a9a-aae2-ef96a78a09cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "58949/58949 [==============================] - 72s 1ms/step - loss: 0.2225 - val_loss: 0.2633\n",
      "Epoch 2/5\n",
      "58949/58949 [==============================] - 71s 1ms/step - loss: 0.2097 - val_loss: 0.2633\n",
      "Epoch 3/5\n",
      "58949/58949 [==============================] - 71s 1ms/step - loss: 0.2097 - val_loss: 0.2633\n",
      "Epoch 4/5\n",
      "58949/58949 [==============================] - 71s 1ms/step - loss: 0.2097 - val_loss: 0.2633\n",
      "Epoch 5/5\n",
      "58949/58949 [==============================] - 71s 1ms/step - loss: 0.2097 - val_loss: 0.2633\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x226927e5d50>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of layers\n",
    "\n",
    "model3 = Sequential()\n",
    "model3.add(Input(shape=(None, numfeatures)))\n",
    "model3.add(Dense(50))\n",
    "model3.add(Dense(50))\n",
    "model3.add(Dense(50))\n",
    "model3.add(Dense(2))\n",
    "model3.compile(optimizer = 'adam', loss = 'mae')\n",
    "\n",
    "model3.fit(features_train, target_train, batch_size = 300,\\\n",
    "          validation_split = 0.20, epochs = 5, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3cf142ae-194d-4f02-b7aa-a62f58b8fc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.save('model3.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e08fba1-7a54-40dc-b38f-2f052091d7c5",
   "metadata": {},
   "source": [
    "Looking at the above, it doesn't seem like any of the parameters evaluated make a real difference to the validation loss. These results suggests that the values we saw initially (in model 1, v1) are likely close to optimal for this type of model. Let's compare the error we're seeing from our predictions to the experimental error reported in the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d14fa4b3-facc-4ff3-a18c-43c0407e9083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean experimental error is 0.1335657449954053\n"
     ]
    }
   ],
   "source": [
    "# compare MAE with the experimental error in the original dataset\n",
    "error_cols = [col for col in ogdata.columns if 'error' in col]\n",
    "print(\"The mean experimental error is\", ogdata[error_cols].mean().mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1327d6-f2a8-491f-a05f-e598e1b4876a",
   "metadata": {},
   "source": [
    "If the measured MAE holds up for our test set, the error will only be slightly higher than that inherent to the data itself.\n",
    "\n",
    "<hr>\n",
    "\n",
    "Let's see how the model does with the test sequences we put aside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "614a90e2-4bc8-46ba-bc05-1f648a08072f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  base base_-2 base_-1 base_1 base_2  position paired  2A3_react  DMS_react\n",
      "0    A       G       G      A   None         3      P        0.3        0.3\n",
      "1    A       G       A      C   None         4      P        0.3        0.3\n",
      "2    A       C       G      C   None         7      P        0.3        0.3\n",
      "3    A       C       G      G   None        12      U        0.3        0.3\n",
      "4    A       G       U      G   None        15      U        0.3        0.3\n"
     ]
    }
   ],
   "source": [
    "test_seqs.reset_index(inplace = True)\n",
    "\n",
    "dfA_neighbors = populate_neighbors('A', distance_list, test_seqs['sequence'])\n",
    "dfC_neighbors = populate_neighbors('C', distance_list, test_seqs['sequence'])\n",
    "dfG_neighbors = populate_neighbors('G', distance_list, test_seqs['sequence'])\n",
    "dfU_neighbors = populate_neighbors('U', distance_list, test_seqs['sequence'])\n",
    "\n",
    "ssViennaDF_test = viennaPU(viennaDF, test_seqs['sequence_id'])\n",
    "\n",
    "data_test = ogdata.loc[ogdata['sequence_id'].isin(test_seqs['sequence_id'])]\n",
    "twoA3reactDF_test = get_reactivity(data_test, '2A3')\n",
    "dmsReactDF_test = get_reactivity(data_test, 'DMS')\n",
    "\n",
    "columns = ['position', 'paired', '2A3_react', 'DMS_react']\n",
    "source_dict = {'position': test_seqs['sequence'], 'paired': ssViennaDF_test,\\\n",
    "               '2A3_react': twoA3reactDF_test, 'DMS_react': dmsReactDF_test}\n",
    "\n",
    "dfG_props = populate_properties('G', columns, source_dict)\n",
    "dfA_props = populate_properties('A', columns, source_dict)\n",
    "dfC_props = populate_properties('C', columns, source_dict)\n",
    "dfU_props = populate_properties('U', columns, source_dict)\n",
    "\n",
    "# combine all of the by-base property dataframes\n",
    "df_props = pd.concat([dfA_props, dfC_props, dfG_props, dfU_props], keys = ['A', 'C', 'G', 'U'])\n",
    "df_props.index.rename(['base', 'index'], inplace = True)\n",
    "df_props.reset_index(level = 0, inplace = True)\n",
    "df_props.reset_index(inplace = True, drop = True)\n",
    "\n",
    "# combine all of the neighbor dataframes\n",
    "df_neighbors = pd.concat([dfA_neighbors, dfC_neighbors, dfG_neighbors, dfU_neighbors],\\\n",
    "                         keys = ['A', 'C', 'G', 'U'])\n",
    "df_neighbors.index.rename(['base', 'index'], inplace = True)\n",
    "df_neighbors.reset_index(level = 0, inplace = True)\n",
    "df_neighbors.reset_index(inplace = True, drop = True)\n",
    "\n",
    "# merge both into one\n",
    "df = df_neighbors.join(df_props, rsuffix='_props')\n",
    "\n",
    "\n",
    "# check that base and base_props are the same and drop base_props\n",
    "\n",
    "if len(df['base'].compare(df['base_props'])) == 0:\n",
    "    df.drop('base_props', axis =1, inplace = True)\n",
    "    print(df.head())\n",
    "else:\n",
    "    print(\"Oops, something went wrong when making the dataframe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ed3a47fc-175e-4371-96b2-335fa1555ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle test dataframe\n",
    "df.to_pickle('df_test.pkl')\n",
    "df_test = df\n",
    "# df_test = pd.read_pickle('df_test.pkl')\n",
    "\n",
    "# prepare for model input/output\n",
    "\n",
    "# features\n",
    "features_test = df_test.drop(['2A3_react', 'DMS_react'], axis = 1)\n",
    "features_test = pd.get_dummies(features_test)\n",
    "features_test = features_test.astype(np.float32)\n",
    "\n",
    "# targets\n",
    "target_test = df_test[['2A3_react', 'DMS_react']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "86888c77-3072-422a-b0bc-128b4cfd0f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "230254/230254 [==============================] - 138s 599us/step\n",
      "MAE of test set: tf.Tensor(\n",
      "[2.6784837e-04 3.0627847e-04 2.9572845e-04 ... 7.2717667e-05 4.1604042e-05\n",
      " 2.4875998e-04], shape=(7368120,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# load model 1 with larger batch size\n",
    "model = keras.models.load_model('model1.keras')\n",
    "predictions = model.predict(features_test)\n",
    "mae = keras.metrics.mae(target_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d4b60f8a-a7c8-461d-87ba-b5b864bb80f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE of test set: 0.220396\n"
     ]
    }
   ],
   "source": [
    "print(\"MAE of test set:\", np.mean(mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4345434-7a78-44a1-b0fb-5d3cbd9c438a",
   "metadata": {},
   "source": [
    "MAE values hold up for the test set as well."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
